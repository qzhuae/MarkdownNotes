# COMP5331 KDD

## Data Warehouse

`Online Analytical Processing` OLAP

Data Warehouse stores pre-computed results to have __fast query response__

Parts are bought from suppliers and then sold to customers at a sale price SP

| part        | supplier         | customer  | SP |
| :-------------: |:-------------:| :-----:| :---: |
|p1|s1|c1|4|
|p3|s1|c2|3|
|p2|s3|c1|7|
|...|...|...|...|

Consider a data cube with dimensions, c, p, s.

[Image Placeholder]

__Example 1__

Select part, customer, SUM(SP) from table T group by part, customer

We can also use AVG(SP), MAX(SP), MIN(SP)

__Example 2__

Select customer, SUM(SP) from table T group by customer

`Materialize` all views wastes a lot of space. 

* Materialize the top view only

* Materialize the top view and the view for ps only

Gain({view for ps, top view}, {top view}) = 5.2 * 3 = 15.6

`Selective Materialization Problem`

We can select a set V of k views such that Gain( V \cup {top view}, {top view}) is maximized

This problem is NP-Hard. `Selective Materialization Decision Problem` SMD

Given an integer k and a real number J, we want to find a set V of k views such that Gain(V \cup {topview}, {topview}) is at least J

This problem is NP-Hard.

`Exact Cover by 3-Sets` (XC) NP-Complete

Instance: Set X with 3q elements, and a collection C of size 3 subsets of X

Question: Does C contain an exact cover for X? i.e. a subcollection C' \in C such that every element of X occurs in exactly one set of C'

Problem XC can be transformed to Problem SMD

- Create a root node with size 200 Level 1

- Create a bottom node with size 1 Level 4

- For each element x in X, 
  - Create node N_x with size 50 
  - Create an edge between N_x and the bottom node

- For each element a \in C where a = (x,y,z)
  - Create a node N_a with size 100 at level 2
  - Create an edge between N_a and the root
  - Create an edge between N_a and N_x
  - Create an edge between N_a and N_y
  - Create an edge between N_a and N_z
 
- Set k = q, J = 400q

 - [ ] We need to verify that solving the problem SMD is equal to solving problem XC. That means Problem SMD is NP-HARD.

P21 - P22?

Greedy Algorithm
k is number of views to be materialized

v is a view, S is a set of views which are selected to be materialization as B(v,S) = Gain(s \cup {v}, S)
- S \leftarrow{top view}
- For i = 1 to k
  - Select that view v not in S such that B(v,S) is maximized;
  - S \leftarrow S \sup \cap {v}
 - Result S is greedy selection

 - [ ] Example on Lecture Note P40
 
 Ratio = Greedy/Optimal is lower bounded by 0.63 
 
 - [ ] See proof from the paper
 
 ## Data Mining over Data Stream
 
 - Association
 
 - Clustering
 
 - Classification
 
 `Data Streams` unbounded data instead of static data, with real-time processing
 
 | |Traditional Data Mining| Data Stream Mining|
 | --- | --- | ---|
 |Data Type| Static Data of Limited Size | Dynamic Data of Unlimited Size, arriving at high speed| 
 | Memory | Limited | More Challenging | 
 | Efficiency | Time-consuming | Efficient |
 | Output | Exact Answer | Approximate or Exact|
 
 
 Each point in the data streams is a transaction
 
 - Obtain data mining results from all data points read so far. Entire Data Streams
 - Obtain data mining results over a sliding window.
 
 Let N be the length of the data streams. Let s be the support threshold (in fraction)
 
 `Frequent Item over Data Streams`
 
 - Let N be the length of the data streams
 - Let s be the support threshold (in fraction) e.g. 20%
 
 Problem: Want to find all items with frequency >= sN or approximate
 
 `False Positive` classified as true (frequent) but in fact is false (infrequent)
 
 `False Negative` classified as false (infrequent) but in fact is true (frequent)
 
 If no false positives, all true infrequent items are classified as infrequent items in the algorithm output. If no false negatives, all true frequent items are classified as frequent items in the algorithm output.
 
 Introduce an input error parameter \epsilon. Denote N as total # of occurrences of items.
 
 Say N = 20, \epsilon = 0.2, \epsilon N = 4.
 
 |Item|True Frequency|Estimated Frequency|Diff. D| D \leq eN?|
 |---|---|---|---|---|
 |I_1|10|10|0|Yes|
 |I_2|8|4|4|Yes|
 |I_3|12|10|2|Yes|
 |...|...|...|...|...|
 
 `$\epsilon$-deficient synopsis`
 
 Denote N as the current length of the stream or total # of occurrences of items.
 
 Let \epsilon be an input parameter or a real number from 0 to 1.
 
 An algorithm maintains an `$\epsilon$-deficient synopsis` if its output satisfies the following:
 
 - Condition 1: There is no false negatives, i.e., All true are classified as true.
 - Condition 2: The difference between the estimated frequency and the true frequency is at most $\epsilon$ N
 - Condition 3: All items whose true frequencies less than $(s - \epsilon)N$ are classified as infrequent items.
 
 ### Frequent Pattern Mining Over Entire Data Streams
 
 - Sticky Sampling Algorithm
 - Lossy Counting Algorithm
 - Space-Saving Algorithm
 
 __Sticky Sampling Algorithm__
 
 Input: support threshold s, error param \epsilon, confidence param \delta, unbounded data
 Stored in memory: Statistics of Items
 Output: Frequent/ Infrequent Items
 
 Sampling Rate r varies over the lifetime of a stream
 
 Confidence param \delta is a small real number
 
 Denote t = \ceiling 1/\epsilon \ln (s^-1 \delta^-1)
 
 For example, 
 
 |Data #| sampling rate r|
 |--- | ---|
 |1~2t|1|
 |2t+1~4t|2|
 |4t+1~8t|4|
 |...|...|
 
 Algorithm:
 - S: empty list and contain element e and estimated frequency f
 - When data e arrives,
    - If e exists in S, increment f in (e,f)
    - If e does not exist in S, add entry (e,1) with prob. 1/r
 - Just after r changes,
    - For each entry (e,f),
        - Repeatedly toss a coin with P(head) = 1/r until the outcome of the coin toss is head
        - If the outcome of the toss is tail, Decrement f in (e,f)
        - If f = 0, delete the entry (e,f)
 - Output a list of imtes where f + \epsilon N >= sN
 
 - [ ] Analysis
 - [ ] \epsilon-deficient synopsis, SS computes an \epsilon deficient synopsis with prob. at least 1 - \delta
 - [ ] Memory Consumption, SS occupies at most \ceiling 2/\epsilon\ln (s^-1\delta^-1)
 
 __Lossy Counting Algorithm__
 
 Input: Suport Threshold s, Error Param \epsilon
 
 Bucket with width w = \ceiling \frac{1}{\epsilon}
 Bucket b_current = \ceiling \frac{N}{w}, where N is current length of stream
 
 __Algorithm__
 
 - D: Empty Set, will contain (element e, Frequency of element since inserted f, Max possible error in f \Delta)
 - When data e arrives: 
    - If e exists in D, increment f in (e,f,\Delta)
    - If e does not exist in D, add entry (e, 1, b_current - 1)
 - Remove some entries in D whenever, N == 0 mod w, i.e. Whenver it reaches the bucket boundary
    - (e,f,\Delta) is deleted if f + \Delta \leq b_current
 - Output a list of items where f + \epsilon N >= sN
 
 - [ ] Analysis
 - [ ] \epsilon-deficient synoposis
 - [ ] Occupies at most \ceiling 1/\epsilon \log(\epsilon N) entries
 
 Notice the effect of N on memory consumption Lecture Note P31-33
 
 __Space-Saving Algorithm__
 
 Input: Momery parameter M, Support Threshold s
 
 Denote M as the greatest number of possible entries stores in the memory.
 
 - [ ] What is the difference between definition of f in Sticky Sampling and Space Saving
 
 __Algorithm__
 
 - D: Empty Set, will contain (element e, Frequency of element since entry inserted f, Max. possible error in f \Delta)
 - p_e = 0
 - When data e arrives,
    - If e exists in D, increment f in (e,f,\Delta)
    - If e does not exist in D,
        - If the size of D == M, 
            - p_e \leftarrow \min_{e\in D} \{f+\Delta\}
            - Remove all entries e where f + \Delta \leq p_e
        - Add entry (e,1,p_e)
 - Output a list of items where f + \Delta \geq sN
 
 - [ ] Analysis 
 - [ ] Greatest Error: Denote E as the greatest error in any estimated frequency. E \leq 1/M
 - [ ] \epsilon-deficient synopsis: Space Saving computes an \epsilon-deficient synopsis if E \leq \epsilon
 
 
 
 
