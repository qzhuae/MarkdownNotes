# COMP5331 KDD

## Data Warehouse

`Online Analytical Processing` OLAP

Data Warehouse stores pre-computed results to have __fast query response__

Parts are bought from suppliers and then sold to customers at a sale price SP

| part        | supplier         | customer  | SP |
| :-------------: |:-------------:| :-----:| :---: |
|p1|s1|c1|4|
|p3|s1|c2|3|
|p2|s3|c1|7|
|...|...|...|...|

Consider a data cube with dimensions, c, p, s.

[Image Placeholder]

__Example 1__

Select part, customer, SUM(SP) from table T group by part, customer

We can also use AVG(SP), MAX(SP), MIN(SP)

__Example 2__

Select customer, SUM(SP) from table T group by customer

`Materialize` all views wastes a lot of space. 

* Materialize the top view only

* Materialize the top view and the view for ps only

Gain({view for ps, top view}, {top view}) = 5.2 * 3 = 15.6

`Selective Materialization Problem`

We can select a set V of k views such that Gain( V \cup {top view}, {top view}) is maximized

This problem is NP-Hard. `Selective Materialization Decision Problem` SMD

Given an integer k and a real number J, we want to find a set V of k views such that Gain(V $\cup$ {topview}, {topview}) is at least J

This problem is NP-Hard.

`Exact Cover by 3-Sets` (XC) NP-Complete

Instance: Set X with 3q elements, and a collection C of size 3 subsets of X

Question: Does C contain an exact cover for X? i.e. a subcollection $C' \in C$ such that every element of X occurs in exactly one set of C'

Problem XC can be transformed to Problem SMD

- Create a root node with size 200 Level 1

- Create a bottom node with size 1 Level 4

- For each element x in X, 
  - Create node N_x with size 50 
  - Create an edge between N_x and the bottom node

- For each element a \in C where a = (x,y,z)
  - Create a node N_a with size 100 at level 2
  - Create an edge between N_a and the root
  - Create an edge between N_a and N_x
  - Create an edge between N_a and N_y
  - Create an edge between N_a and N_z
 
- Set k = q, J = 400q

 - [ ] We need to verify that solving the problem SMD is equal to solving problem XC. That means Problem SMD is NP-HARD.

P21 - P22?

Greedy Algorithm
k is number of views to be materialized

v is a view, S is a set of views which are selected to be materialization as B(v,S) = Gain(s \cup {v}, S)
- S \leftarrow{top view}
- For i = 1 to k
  - Select that view v not in S such that B(v,S) is maximized;
  - S \leftarrow S \sup \cap {v}
 - Result S is greedy selection

 - [ ] Example on Lecture Note P40
 
 Ratio = Greedy/Optimal is lower bounded by 0.63 
 
 - [ ] See proof from the paper
 
 ## Data Mining over Data Stream
 
 - Association
 
 - Clustering
 
 - Classification
 
 `Data Streams` unbounded data instead of static data, with real-time processing
 
 | |Traditional Data Mining| Data Stream Mining|
 | --- | --- | ---|
 |Data Type| Static Data of Limited Size | Dynamic Data of Unlimited Size, arriving at high speed| 
 | Memory | Limited | More Challenging | 
 | Efficiency | Time-consuming | Efficient |
 | Output | Exact Answer | Approximate or Exact|
 
 
 Each point in the data streams is a transaction
 
 - Obtain data mining results from all data points read so far. Entire Data Streams
 - Obtain data mining results over a sliding window.
 
 Let N be the length of the data streams. Let s be the support threshold (in fraction)
 
 `Frequent Item over Data Streams`
 
 - Let N be the length of the data streams
 - Let s be the support threshold (in fraction) e.g. 20%
 
 Problem: Want to find all items with frequency >= sN or approximate
 
 `False Positive` classified as true (frequent) but in fact is false (infrequent)
 
 `False Negative` classified as false (infrequent) but in fact is true (frequent)
 
 If no false positives, all true infrequent items are classified as infrequent items in the algorithm output. If no false negatives, all true frequent items are classified as frequent items in the algorithm output.
 
 Introduce an input error parameter \epsilon. Denote N as total # of occurrences of items.
 
 Say N = 20, \epsilon = 0.2, \epsilon N = 4.
 
 |Item|True Frequency|Estimated Frequency|Diff. D| D \leq eN?|
 |---|---|---|---|---|
 |I_1|10|10|0|Yes|
 |I_2|8|4|4|Yes|
 |I_3|12|10|2|Yes|
 |...|...|...|...|...|
 
 `$\epsilon$-deficient synopsis`
 
 Denote N as the current length of the stream or total # of occurrences of items.
 
 Let \epsilon be an input parameter or a real number from 0 to 1.
 
 An algorithm maintains an `$\epsilon$-deficient synopsis` if its output satisfies the following:
 
 - Condition 1: There is no false negatives, i.e., All true are classified as true.
 - Condition 2: The difference between the estimated frequency and the true frequency is at most $\epsilon$ N
 - Condition 3: All items whose true frequencies less than $(s - \epsilon)N$ are classified as infrequent items.
 
 ### Frequent Pattern Mining Over Entire Data Streams
 
 - Sticky Sampling Algorithm
 - Lossy Counting Algorithm
 - Space-Saving Algorithm
 
 __Sticky Sampling Algorithm__
 
 Input: support threshold s, error param \epsilon, confidence param \delta, unbounded data
 Stored in memory: Statistics of Items
 Output: Frequent/ Infrequent Items
 
 Sampling Rate r varies over the lifetime of a stream
 
 Confidence param \delta is a small real number
 
 Denote t = \ceiling 1/\epsilon \ln (s^-1 \delta^-1)
 
 For example, 
 
 |Data #| sampling rate r|
 |--- | ---|
 |1~2t|1|
 |2t+1~4t|2|
 |4t+1~8t|4|
 |...|...|
 
 Algorithm:
 - S: empty list and contain element e and estimated frequency f
 - When data e arrives,
    - If e exists in S, increment f in (e,f)
    - If e does not exist in S, add entry (e,1) with prob. 1/r
 - Just after r changes,
    - For each entry (e,f),
        - Repeatedly toss a coin with P(head) = 1/r until the outcome of the coin toss is head
        - If the outcome of the toss is tail, Decrement f in (e,f)
        - If f = 0, delete the entry (e,f)
 - Output a list of imtes where f + \epsilon N >= sN
 
 - [ ] Analysis
 - [ ] \epsilon-deficient synopsis, SS computes an \epsilon deficient synopsis with prob. at least 1 - \delta
 - [ ] Memory Consumption, SS occupies at most \ceiling 2/\epsilon\ln (s^-1\delta^-1)
 
 __Lossy Counting Algorithm__
 
 Input: Suport Threshold s, Error Param \epsilon
 
 Bucket with width w = \ceiling \frac{1}{\epsilon}
 Bucket b_current = \ceiling \frac{N}{w}, where N is current length of stream
 
 __Algorithm__
 
 - D: Empty Set, will contain (element e, Frequency of element since inserted f, Max possible error in f \Delta)
 - When data e arrives: 
    - If e exists in D, increment f in (e,f,\Delta)
    - If e does not exist in D, add entry (e, 1, b_current - 1)
 - Remove some entries in D whenever, N == 0 mod w, i.e. Whenver it reaches the bucket boundary
    - (e,f,\Delta) is deleted if f + \Delta \leq b_current
 - Output a list of items where f + \epsilon N >= sN
 
 - [ ] Analysis
 - [ ] \epsilon-deficient synoposis
 - [ ] Occupies at most \ceiling 1/\epsilon \log(\epsilon N) entries
 
 Notice the effect of N on memory consumption Lecture Note P31-33
 
 __Space-Saving Algorithm__
 
 Input: Momery parameter M, Support Threshold s
 
 Denote M as the greatest number of possible entries stores in the memory.
 
 - [ ] What is the difference between definition of f in Sticky Sampling and Space Saving
 
 __Algorithm__
 
 - D: Empty Set, will contain (element e, Frequency of element since entry inserted f, Max. possible error in f \Delta)
 - p_e = 0
 - When data e arrives,
    - If e exists in D, increment f in (e,f,\Delta)
    - If e does not exist in D,
        - If the size of D == M, 
            - p_e \leftarrow \min_{e\in D} \{f+\Delta\}
            - Remove all entries e where f + \Delta \leq p_e
        - Add entry (e,1,p_e)
 - Output a list of items where f + \Delta \geq sN
 
 - [ ] Analysis 
 - [ ] Greatest Error: Denote E as the greatest error in any estimated frequency. E \leq 1/M
 - [ ] \epsilon-deficient synopsis: Space Saving computes an \epsilon-deficient synopsis if E \leq \epsilon
 
 ||\epsilon-deficient synopsis|Memory Consumption|
 |---|---|---|
 |Sticky Sampling| 1 - \delta confidence| \ceiling 2/\epsilon \ln(s^-1\delta^-1)|
 |Lossy Counting| 100% confidence| \ceiling 1/\epsilon log(\epsilon N) |
 |Space Saving| 100% confidence when E \leq \epsilon|M |
 
 Consider s = 0.02, \epsilon = 0.01, \delta = 0.1, N = 10e9.
 
 In space saving algorithm, memory can be very large 4e6 since E \leq 1/M \rightarrow the error is very small
 
 ### Data Streams with Sliding Window 
 
 __Clustering__ BIRCH, advantage: incremental scalable
 
 __Classification__
 
 For Data Streams,
 
 - Impossible to build a decision tree according to all data points
 - Have to 'wait' for all data points
 - Instead, we just read a certain amount of data points and build a decision tree.
 - Problem here is __incorrent information gain__
 
 Idea: Read 'enough' data points before making a decision to split the data points
 
 Denote N as the # data points read so far
 
 - \bar{Gain(A,T)} \rightarrow Gain(A,T) when N becomes large
 
 - \bar{Gain(A,T)}: greatest value
 
 - \bar{Gain(B,T)}: second greatest value
 
 - \delta: a user param \in [0,1]
 
 If \bar{Gain(A,T)} - \bar{Gain(B,T)} > \epsilon, the probaility that Gain(A,T) - Gain(B,T) > 0 is at least 1 - \delta
 
 \epsilon is dependent on \delta and # data points read so far.
 
 `Hoeffding Tree Algorithm`
 
 \epsilon = \sqrt{\frac{R^2\ln(1/\delta)}{2N}} If N is larger, \epsilon is smaller
 
 N total number of data points read so far

 R is the range of Gain(A,T)
 
 R = \log c for Gain(A,T) where c is the total # possible classes in the target attribute
 
 __Algorithm__
 
 - Create a storage for the root node
 - Whenever the data point comes, 
   - For each leaf node,
     - Update the Storage, increment the correspondence counts
     - Calculate Information Gain of each (remaining) attribute, Use the storage to calculate the information gain
     - Find 1) greatest and 2) second greatest value \bar{Gain(A,T)}, \bar{Gain(B,T)}
   - If difference > \epsilon,
     - Perform the splitting according to attribute A
     - For each leaf node, create a storage for this node
 
 ## Web Databases
 
 How to rank the webpages?
 
 Ranking Methods:
 
 - HITS algorithm
 - PageRank algorithm
 
 ### HITS algorithm
 
 HITS is a ranking algorithm which ranks “hubs” and “authorities”. [Beaware of the graphic representation of each]
 
 Each page (vertex) has two weights 1) Authority weight a(v) 2) Hub weight h(v)
 
 A good authority has many edges from good hubs: a(v) = \sum_{u\rightarrow v} h(u)
 
 A good hubs has many outgoing edges to good authorities: h(v) = \sum_{v \rightarrow u} a(v)
 
 Two major steps: Step 1, sampling step; Step 2, iteration step
 
 __Sampling Step__
 
 Given a user query with several terms, collect a set of pages that are very relevant - called the `base set`
 
 How to find base set?
 - We retrieve all webpages that contain the query terms. The set of webpages is called the `root set`.
 - Next, find the `link pages`, which are either pages with a hyperlink to some page in the root set or some page in the root set has hyperlink to these pages
 - All pages found form the `base set`
 
 __Iteration Step__
 
 Goal: to find the base pages that are good hubs and good authorities
 
 Fill in the gaps of Lecture Note P12-17
 
 How to Rank?
 - Rank in descending order of hub only
 - Rank in descending order of authority only 
 - Rank in descending order of the value computed from both hub and authority, e.g., the sum
 
 ### PageRank Algorithm (Google)
 
 Disadvantage of HITS: 
 - We do not know whether hub or authorities are more important for ranking
 
 Advantage of PageRank:
 - PageRank involves only one concept for ranking
 
 Makes use of Stochastic Approach to rank the pages
 
 Fill in the gaps of Lecture Note P22-28
 
 

## Course Project Presentation Nov 29 2pm

### General Comments on MStream Algorithm

- MStream algorithm favors a high number of clusters.
- Each cluster is small with a few document.
- For example, on the reddit data set, we have 539 clusters instead of 4 labled clusters. This could be bad because ... NMI accuracy is zero in this case. The algorithm does not do what it supposed to do.
- Because we are trying to do unsupervised clustering here. 539 clusters might represent better representation than the original 4 clusters.
- The cluster it created can provided valuable insights into the short text stream. And the clusters we retrieved are semantically meaningful in most of the cases.

When doing the experimentation, we noticed an interesting fact, that Mstream algorithm favored a high number of clusters, where each cluster is small with few documents. When we tested on the Reddit data set, the original data was labeled into 4 clusters. However, since Mstream has no prior knowledge about this, it generated what it “thought” would be the best number of clusters with the data, which was 539. Hence, the NMI value was very small (approximately 0).

One can argue about this being an advantage or a disadvantage. On one side, one can say that the NMI accuracy was very bad (approximalely 0) and hence the algorithm did not do what it was supposed to do. However, we would be confusing clustering with classification. It is correct that MStream failed in classification, but perhaps the 539 generated clusters are actually better representations than original 4 clusters in the labeled data! Hence, you can argue that this is a good thing.

We observed that the MStream algorithm favors clustering a high number of clusters, where each cluster is small with few document. The clusters it created can provide valuable insights into the short text stream. For instance, the clusters we retrieved are semantically meaningful in most of the cases.

### Other Approaches Tried

### Model-based vs. Non-model-based, Parametric over non-parametric

- Parametric Model based approach, and some parameters are determined by sampling. Sampling can provide a good parameter estimation. 

Model-based approach assumes a certain bayesian point of view. 

- Non-parametric non-model-based approach

A non-model-based frequentists view might miss out on association between consecutive words inside a same data stream. 

To have more degree of flexibility.It is also possible to extend the non-model-based approaches to do short text clustering, for example, density-based clustering. 

However, the non-model-based methods, without restriction to a model, could potentially capture less semantically meaningful clus-ters. Because certain association between words should be assumed. 

So,  while  extending  non-model-based  algorithms,  two  key  problems  should  be  kept in mind.  

One is to find a good representation of the corpus, and the other is to deal withthe high dimensionality of words.

One of the possible ways to represent words would be using word2vec embedding provided by Google which has a good measure of “closeness” or distance between words and its resulthas meaningful semantic representation because of the underlying method the word vectorsare obtained.  

And certainly, we can obtain a word2vec embedding tailor-made for our corpusand run clustering algorithms upon the word vectors.

The sparsity issue or high dimensionality problem is usually mitigated by subspace clusteringand dimensionality reduction. In this case, we should be careful about which dimensions are chosen such that the clusters are semantically meaningful. 

If density-based method is used, we should determine suitable definition of density.

### Deep Learning Based Models

We found that no deep learning model has been compared in the paper, which turns out tobe a common phenomena as deep learning approach is generally less favored in the realm ofunsupervised clustering.  

We suggest the following weaknesses of deep learning models whichprevent them from being widely adopted in the clustering problem.

•Lack of well-defined differentiable loss function. As the optimization process relieson  the gradient  from  the loss  computed  for  each  training  step,  deep  learning  modelis generally less favorable for clustering as it is difficult to design a differentiable lossfunction for unsupervised clustering problem such as cross-entropy for classification.

•Time  consuming Deep  learning  algorithm  usually  take  hundreds  of  iterations  toobtain satisfactory performance, which makes it inferior to traditional clustering tech-niques which are much more efficient. In addition, as the short text stream changesrapidly over time, deep learning model may need to be re-trained frequently to incor-porate the information of new documents, which is impractical. 

In summary, deep-learing-based models are hard to train because it is hard to find a goodloss function such that different semantic meanings are separated.  Also, as deep-learning-based models are slow to train in general, deep-learning-based methods are not suitable forthe case that our dataset is dynamic as in short text streams in Twitter or Reddit.

On the other hand, there are certainly perks of adapting certain deep-learning-based models.For example, [21] shows that CNN-based method can have great NMI accuracy (69%) onSearchSnippets dataset. This resonates with the fact that deep-learning-based model can betrained so that it applies to a certain dataset really well. Other advantage of using deep-learning-based  model,  especially  auto-encoder-based  model,  is that the feature  extracted from these models usually provide you with better insights about the bases of how yourclusters are separated.

### Conclusion

- Mstream algorithm works
- Perform slightly worse than the code provided along the KDD paper 
- Best Hyperparameters for us are also different
- Two new datasets - Google Play Store Apps and Reddit Posts
- Researched and Analyzed other models

We have verified that the MStream algorithm proposed is indeed able to cluster short texts. However, our implementation performs slightly worse than the code provided along with the paper. This is possibly because of different hyper-parameters used and some nuances in the implementation. In addition, we played with the codes on two new datasets - Google Play Store Apps and Reddit Posts. 

Finally, we also discussed about related works on this topics and some pros and cons of other possible models. 

### Some of My References:

https://www.google.com.hk/search?q=Can+you+cluster+text+stream+using+deep+learning+based+models%3F&rlz=1C1GCEB_enHK820HK820&oq=Can+you+cluster+text+stream+using+deep+learning+based+models%3F&aqs=chrome..69i57.13711j0j7&sourceid=chrome&ie=UTF-8

https://www.hindawi.com/journals/mpe/2017/8310934/

https://www.quora.com/How-can-deep-learning-be-applied-to-data-stream-clustering

https://arxiv.org/abs/1501.03084

https://ieeexplore.ieee.org/document/7838144

<!--stackedit_data:
eyJoaXN0b3J5IjpbLTYwMjA5NjgzNF19
-->