


> Written with [StackEdit](https://stackedit.io/).
# COMP5112 Parallel Programming

## Introduction to Parallel Computing


From 1986 – 2002, microprocessors were speeding like a rocket, increasing in performance an average of 50% per year. Since then, it’s dropped to about 20% increase per year.

[An intelligent solution] Instead of designing and building faster microprocessors, put multiple processors on a single integrated circuit.

Why do we need ever-increasing performance?
Computational power is increasing, but so are our computation problems and needs. Problems we never dreamed of have been solved, such as decoding the human genome. More complex problems are still waiting to be solved. Such as,  Climate Modeling, Protein folding, Drug discovery, Energy Research, Data analysis.

Up to now, performance increases have been attributable to increasing density of transistors. But there are inherent problems:
- Denser transistors -> faster processors
- Faster processors -> increased power consumption
- Increased power consumption -> increased heat
- Increased heat -> unreliable processors

Solution is to move away from single-core systems to multicore processors. "core" = central processing unit (CPU) by introducing parallelism

We can rewrite serial programs so that they are parallel. Write translation programs that automatically convert serial programs into parallel programs. But this is difficult to do and success has been limited.

More problems with translation: 
- Some coding constructs can be recognized by an automatic program generator, and converted to a parallel construct.
- However, it is likely that the results will be a very inefficient program.
- Sometimes the best parallel solution is to step back and devise an entirely new algorithm.

Example: compute n values and add them together
Serial Solution:

    sum = 0;
    for (i = 0; i < n; i ++){
		x = Compute_next_value();
		sum += x;
	}

Suppose we have p cores, p << n. Each core performs a partial sum of approximately n/p values.

    my_sum = 0;
    my_first_i = ;
    my_last_i = ;
    for (my_i = my_first_i; my_i < my_last_i; my_i++){
	    my_x = Compute_next_value();
	    my_sum += my_x;}

After each core completes execution of the code, a private variable my_sum contains the sum of the values computed by its calls to Compute_next_value(). Once all the cores are done computing their private my_sum, they form a global sum by sending results to a designated “master” core which adds the final result.

    If (I'm the master core){
	    sum = my_x;
	    for each core other than myself{
		    receive value from core;
		    sum += value;
		}
	} else {
		send my_x to the master;
	}

To have a better parallel algorithm, do not make the master core do all the work. Share it among the other cores. Pair the cores so that core 0 adds its result with core1's result. Core 2 adds its result with core 3's result, etc. Work with odd and even numbered pairs of cores.

Now repeat the process with only the evenly ranked cores. Core 0 adds result from core 2. Core 4 adds the result from core 6, etc. Now cores divisible by 4 repeat the process, and so forth, until core 0 has the final result.

In the first example, the master core performs 7 receives and 7 additions. In the second example, the master core performs 3 receives and 3 additions. [Know why? Refer to P26] The improvement is more than a factor of 2. 

The difference is more dramatic with a larger number of cores. If we have 1000 cores, we may improve by factor of 100.

**How do we write parallel programs?**
- `Task parallelism` Partition various tasks solving the problem among the cores. Tasks: 1 receiving and 2 addition.

    If (I'm the master core) {
        sum = my_x;
        for each core other than myself{
    	    receive value from core;
    	    sum += value;
    	 }
    } else send my_x to the master; 

- `Data parallelism` Partition the data used in solving the problem among the cores. Each core carries out similar operations on its part of the data.

    sum = 0;
    for (i = 0; i < n; i++){
        x = Compute_next_value();
        sum += x;
    }

### Coordination
- Cores usually need to coordinate their work.
- `Communication` one or more cores send their current partial sums to another core
- `Load balancing` share the work evenly among the cores so that one is not heavily loaded.
- `Synchronization` because each core works at its own pace, make sure cores do not get too far ahead of the rest

### Types of parallel systems
- Shared-memory
	- The cores can share access to the computer's memory
	- Coordinate the cores by having them examine and update shared memory locations
- Distributed-memory
	- Each core has its own, private memory
	- The cores must communicate explicitly by sending messages across a network.

### Terminology
- `Concurrent computing` in a program multiple tasks can be in progress at any instant.
- `Parallel computing` in a program multiple tasks cooperate closely to solve a problem.
- `Distributed computing` a program may need to cooperate with other programs to solve a problem.

## Review on Computer Hardware and Operating Systems

### The Von Neumann Architecture

**Main memory**
 - It is a collection of **locations**, each of which is capable of storing both instructions and data. 
- Every location consists of an **address**, which is used to access the location, and the **contents** of the location.

**Central Processing Unit (CPU)**
- Control unit - responsible for deciding which instruction in a program should be executed
- Arithmetic and Logic Unit (ALU) - responsible for executing the actual instructions.

Key Terms:
- **Register** very fast storage, part of the CPU
- **Program counter** stores address of the next instruction to be executed
- **Bus** wires that connect the CPU and memory.

`The Von Neumann Bottleneck`

The separation of memory and CPU
- The limited storage capacity of the CPU means that large amounts of data and instructions must be transferred from the memory
- The interconnect determines the rate at which instructions and data can be accessed
- The CPU can execute instructions orders of magnitude faster than memory access

`Process`
- An instance of a computer program that is being executed.
- Components:
	- The executable machine language program
	- A block of memory
	- Descriptors of resources the OS has allocated to the process
	- Security information
	- Information about the state of the process

Multitasking
- Gives the illusion that a single processor system is running multiple programs simultaneously.
- Each process takes turns running. `time slice`
- After its time is up, it waits until it has a turn again. `blocks`

Threading
- Threads are contained within processes
- They allow programmers to divide their programs into (more or less) independent tasks
- The hope is that *when one thread blocks because it is waiting on a resource, another will have work to do and can run*

A process and two threads: the "master" thread
- starting a thread is called `forking`
- terminating a thread is called `joining`

### Modifications to the Von Neumann Model

`Caches`
- A collection of memory locations that can be accessed in less time than some other memory locations
- A CPU cache is typically located on the same chip, or one that can be accessed much faster than ordinary memory

`Locality` The same or nearby locations are accessed frequently
`Spatial locality`: accessing a nearby location
`Temporal locality`: accessing in the near future

Levels of Caches, Cache Hit, Cache Miss

When a CPU writes data to cache, the value in cache may be inconsistent with the value in the main memory. 
- `Write-through` caches handle this by updating the data in main memory at the time it is written to cache.
- `Write-back` caches mark data in the cache as dirty. When the cache line is replaced by a new cache line from memory, the dirty line is written to memory.

**Cache Mapping**
- `Full associative` a new line can be placed at any location in the cache.
- `Direct mapped` each cache line has a unique location in the cache to which it will be assigned
- `n-way set associative` each cache line can be place in one of n different locations in the cache.

**Cache Eviction**
Caches are much smaller than main memory. When the cache is full, bringing a new line in memory needs to replace or evict a line in the cache. Common cache eviction policies include **LRU/MRU (Least/Most Recently Used)** and **LFU (Least Frequently Used)**.

### Virtual Memory
- If we run a very large program or a program that accesses very large data sets, all of the instructions and data may not fit into main memory.
- Virtual memory functions as a cache for secondary storage.
- `Swap space` an area of secondary storage that keeps the inactive (parts of) running programs.
- `Pages` blocks of data and instructions. Most systems have a fixed page size that currently ranges from 4 to 16 kilobytes.

When a program is compiled its pages are assigned `Virtual Page Numbers`

When the program is run, a table is created that maps the virtual page numbers to physical addresses.

A `page table` is used to translate the virtual address into a physical address.

Virtual Address Divided into Virtual Page Number and Byte Offset

`Translation-lookaside Buffer` TLB
- Using a page table has the potential to significantly increase each program's overall run-time.
- TLB is a special address translation cache in the processor.
- It caches a small number of entries (typically 16–512) from the page table in very fast memory.
- `Page Fault` attempting to access a valid physical address for a page in the page table but the page is only stored on disk.

`Instruction Level Parallelism (ILP)`
- Attempts to improve processor performance by having multiple processor components or functional units simultaneously executing instructions.
- `Pipelining`  functional units are arranged in stages.
	- Divide the floating point adder into 7 separate pieces of hardware or functional units.
	- First unit fetches two operands, second unit compares exponents, etc.
	- Output of one functional unit is input to the next.
	- One floating point addition still takes 7 nanoseconds.
	- But 1000 floating point additions now takes 1006 nanoseconds!
- `Multiple issue` multiple instructions can be simultaneously initiated.
	- Multiple issue processors replicate functional units and try to simultaneously execute different instructions in a program.
	- `static multiple issue` functional units are scheduled at compile time.
	- `dynamic multiple issue` functional units are scheduled at run-time. `superscalar` 

`Speculation`
- In order to make use of multiple issue, the system must find instructions that can be executed simultaneously.
- In speculation, the compiler or the processor makes a guess about an instruction, and then executes the instruction on the basis of the guess.

`Hardware multithreading`
- There aren't always good opportunities for simultaneous execution of different threads.
- Hardware multithreading provides a means for systems to continue doing useful work when the task being currently executed has stalled. Ex., the current task has to wait for data to be loaded from memory.
- `Fine-grained` the processor switches between threads after each instruction, skipping threads that are stalled
	- Pros: potential to avoid wasted machine time due to stalls
	- Cons: a thread that is ready to execute a long sequence of instructions may have to wait to execute every instruction.
- `Coarse-grained` only switches threads that are stalled waiting for a time-consuming operation to complete.
	- Pros: switching threads doesn't need to be nearly instantaneous
	- Cons: the processor can be idled on shorter stalls, and thread switching will also cause delays.
- `Simultaneous multithreading (SMT)` a variation on fine-grained multithreading. 
- Allows multiple threads to make use of the multiple functional units. 

## Feb 11 Parallel Hardware

SIMD: Single instruction stream, multiple data stream
MIMD: Multiple instruction stream, multiple data stream

###  SIMD:
- Parallelism achieved by dividing data among the processors.
- Applies the same instruction to multiple data items
- Data Parallelism
Drawbacks:
- All ALUs are required to execute the same instruction, or remain idle.
- In traditional design, the must also operate synchronously
- no instruction storage for ALU
- Expensive Synchronization Overhead, tick on the same clock.
- Efficient for large data parallel problems, but not other types of more complex parallel problems.

Vector Processors
- Operate on arrays or vectors of data while conventional CPU’s operate on individual data elements or scalars. (Each ALU has separate register, in this case, we have vector register)
- `Vector registers`: Capable of storing a vector of operands and operating simultaneously on their contents.
- `Vectorized and pipelined functional units`: The same operation is applied to each element in the vector (or pairs of elements).
- `Vector instructions`: Operate on vectors rather than scalars.
- `Interleaved memory` 
	- Multiple “banks” of memory, which can be accessed more or less independently.
	- Distribute elements of a vector across multiple banks, so reduce or eliminate delay in loading/storing successive elements.
- Strided memory access and hardware scatter/gather.
	- The program accesses elements of a vector located at fixed intervals.

Pros:
- Fast
- Easy to use.
- Vectorizing compilers are good at identifying code to exploit.
- Compilers also can provide information about code that cannot be vectorized
	- Helps the programmer re-evaluate code
- High memory bandwidth
- Uses every item in a cache line.

Cons:
- They don't handle irregular data structures as well as other parallel architectures.
- Limited to their ability to handle ever larger problems. (scalability)

### Graphics Processing Units (GPU)

- Real time graphics application programming interfaces or API's use points, lines, and triangles to internally represent the surface of an object.
- A graphics processing pipeline converts the internal representation into an array of pixels that can be sent to a computer screen.
- Several stages of this pipeline (called `shader functions`) are programmable.
- Shader functions are also implicitly parallel, since they can be applied to multiple elements in the graphics stream.
- GPU’s can often optimize performance by using SIMD parallelism.
- The current generation of GPU’s use SIMD parallelism. Although they are not pure SIMD systems. 
	 
The larger a memory size, the more expensive the per unit price.

### MIMD
- Supports multiple simultaneous instruction streams operating on multiple data streams.
- Typically consist of a collection of fully independent processing units or cores, each of which has its own control unit and its own ALU.

### Shared Memory System
- A collection of autonomous processors is connected to a memory system via an interconnection network.
- Each processor can access each memory location.
- The processors usually communicate implicitly by accessing shared data structures.
- Most widely available shared memory systems use one or more multicore processors.

![UMA Multicore](https://lh3.googleusercontent.com/3zN4g2BTIbOnDs-rilJJDU0hH2HPzpAiJbKNjKB0IAXrracnJiUoE2fhqb3odxuPmvMXT9LfVZRc)

![NUMA Multicore](https://lh3.googleusercontent.com/ZBzEWYdE_1DbJpbDPjjxsu90fMH-1Bd26V65dmlis1j5r6BhEu-x-_fWr-EII5f0i7nPrE7fmKrE)

### Distributed Memory System
- Clusters
	- A collection of commodity systems
	- Connected by a commodity interconnection network
- `Nodes` of a cluster are individual computations units joined by a communication network.

**Interconnection network**
- Affects performance of both distributed and shared memory systems.



<!--stackedit_data:
eyJoaXN0b3J5IjpbLTc3MDUyMTIyNiwxNjg0Mjk1MzQxLDg2Nz
I5MjE2NCw0MjcwNDQxMiwtOTA3MjM2NzM4LDE5NDAxNjgzOTIs
MTM0MTI1MTY1NSwxMTc2ODU0NDM5LC0xNDAwNzUwNzM4LC0yNT
kxMDU4MzksLTEyMzk3ODY3MzksLTgyNTEzMzY4MiwtMTkzNDA3
MDExOSwxODI3Mjk4MDYwLC01MDcxODM1ODUsLTE5MTU2OTIwND
UsLTIwNjk1MTYxMzYsMTU4MjU3MTc0OSwxMzY0MzY3OTM0XX0=

-->