


> Written with [StackEdit](https://stackedit.io/).
# COMP5112 Parallel Programming

## Introduction to Parallel Computing


From 1986 – 2002, microprocessors were speeding like a rocket, increasing in performance an average of 50% per year. Since then, it’s dropped to about 20% increase per year.

[An intelligent solution] Instead of designing and building faster microprocessors, put multiple processors on a single integrated circuit.

Why do we need ever-increasing performance?
Computational power is increasing, but so are our computation problems and needs. Problems we never dreamed of have been solved, such as decoding the human genome. More complex problems are still waiting to be solved. Such as,  Climate Modeling, Protein folding, Drug discovery, Energy Research, Data analysis.

Up to now, performance increases have been attributable to increasing density of transistors. But there are inherent problems:
- Denser transistors -> faster processors
- Faster processors -> increased power consumption
- Increased power consumption -> increased heat
- Increased heat -> unreliable processors

Solution is to move away from single-core systems to multicore processors. "core" = central processing unit (CPU) by introducing parallelism

We can rewrite serial programs so that they are parallel. Write translation programs that automatically convert serial programs into parallel programs. But this is difficult to do and success has been limited.

More problems with translation: 
- Some coding constructs can be recognized by an automatic program generator, and converted to a parallel construct.
- However, it is likely that the results will be a very inefficient program.
- Sometimes the best parallel solution is to step back and devise an entirely new algorithm.

Example: compute n values and add them together
Serial Solution:

    sum = 0;
    for (i = 0; i < n; i ++){
		x = Compute_next_value();
		sum += x;
	}

Suppose we have p cores, p << n. Each core performs a partial sum of approximately n/p values.

    my_sum = 0;
    my_first_i = ;
    my_last_i = ;
    for (my_i = my_first_i; my_i < my_last_i; my_i++){
	    my_x = Compute_next_value();
	    my_sum += my_x;}

After each core completes execution of the code, a private variable my_sum contains the sum of the values computed by its calls to Compute_next_value(). Once all the cores are done computing their private my_sum, they form a global sum by sending results to a designated “master” core which adds the final result.

    If (I'm the master core){
	    sum = my_x;
	    for each core other than myself{
		    receive value from core;
		    sum += value;
		}
	} else {
		send my_x to the master;
	}

To have a better parallel algorithm, do not make the master core do all the work. Share it among the other cores. Pair the cores so that core 0 adds its result with core1's result. Core 2 adds its result with core 3's result, etc. Work with odd and even numbered pairs of cores.

Now repeat the process with only the evenly ranked cores. Core 0 adds result from core 2. Core 4 adds the result from core 6, etc. Now cores divisible by 4 repeat the process, and so forth, until core 0 has the final result.

In the first example, the master core performs 7 receives and 7 additions. In the second example, the master core performs 3 receives and 3 additions. [Know why? Refer to P26] The improvement is more than a factor of 2. 

The difference is more dramatic with a larger number of cores. If we have 1000 cores, we may improve by factor of 100.

**How do we write parallel programs?**
- `Task parallelism` Partition various tasks solving the problem among the cores. Tasks: 1 receiving and 2 addition.

    If (I'm the master core) {
        sum = my_x;
        for each core other than myself{
    	    receive value from core;
    	    sum += value;
    	 }
    } else send my_x to the master; 

- `Data parallelism` Partition the data used in solving the problem among the cores. Each core carries out similar operations on its part of the data.

    sum = 0;
    for (i = 0; i < n; i++){
        x = Compute_next_value();
        sum += x;
    }

### Coordination
- Cores usually need to coordinate their work.
- `Communication` one or more cores send their current partial sums to another core
- `Load balancing` share the work evenly among the cores so that one is not heavily loaded.
- `Synchronization` because each core works at its own pace, make sure cores do not get too far ahead of the rest

### Types of parallel systems
- Shared-memory
	- The cores can share access to the computer's memory
	- Coordinate the cores by having them examine and update shared memory locations
- Distributed-memory
	- Each core has its own, private memory
	- The cores must communicate explicitly by sending messages across a network.

### Terminology
- `Concurrent computing` in a program multiple tasks can be in progress at any instant.
- `Parallel computing` in a program multiple tasks cooperate closely to solve a problem.
- `Distributed computing` a program may need to cooperate with other programs to solve a problem.

## Review on Computer Hardware and Operating Systems

`The Von Neumann Architecture`

**Main memory**
 - It is a collection of **locations**, each of which is capable of storing both instructions and data. 
- Every location consists of an **address**, which is used to access the location, and the **contents** of the location.

**Central Processing Unit (CPU)**
- Control unit - responsible for deciding which instruction in a program should be executed
- Arithmetic and Logic Unit (ALU) - responsible for executing the actual instructions.

Key Terms:
- **Register** very fast storage, part of the CPU
- **Program counter** stores address of the next instruction to be executed
- **Bus** wires that connect the CPU and memory.

`The Von Neumann Bottleneck`

The separation of memory and CPU
- The limited storage capacity of the CPU means that large amounts of data and instructions must be transferred from the memory
- The interconnect determines the rate at which instructions and data can be accessed
- The CPU can execute instructions orders of magnitude faster than memory access

`Process`
- An instance of a computer program that is being executed.
- Components:
	- The executable machine language program
	- A block of memory
	- Descriptors of resources the OS has allocated to the process
	- Security information
	- Information about the state of the process

Multitasking
- Gives the illusion that a single processor system is running multiple programs simultaneously.
- Each process takes turns running. `time slice`
- After its time is up, it waits until it has a turn again. `blocks`

Threading
- Threads are contained within processes
- They allow programmers to divide their programs into (more or less) independent tasks
- The hope is that when one thread blocks because it is waiting on a resource, another will have work to do and can run
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE1MDM4NTQ3NSwtMjA2OTUxNjEzNiwxNT
gyNTcxNzQ5LDEzNjQzNjc5MzRdfQ==
-->