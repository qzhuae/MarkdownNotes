


> Written with [StackEdit](https://stackedit.io/).
# COMP5112 Parallel Programming

## Introduction to Parallel Computing


From 1986 – 2002, microprocessors were speeding like a rocket, increasing in performance an average of 50% per year. Since then, it’s dropped to about 20% increase per year.

[An intelligent solution] Instead of designing and building faster microprocessors, put multiple processors on a single integrated circuit.

Why do we need ever-increasing performance?
Computational power is increasing, but so are our computation problems and needs. Problems we never dreamed of have been solved, such as decoding the human genome. More complex problems are still waiting to be solved. Such as,  Climate Modeling, Protein folding, Drug discovery, Energy Research, Data analysis.

Up to now, performance increases have been attributable to increasing density of transistors. But there are inherent problems:
- Denser transistors -> faster processors
- Faster processors -> increased power consumption
- Increased power consumption -> increased heat
- Increased heat -> unreliable processors

Solution is to move away from single-core systems to multicore processors. "core" = central processing unit (CPU) by introducing parallelism

We can rewrite serial programs so that they are parallel. Write translation programs that automatically convert serial programs into parallel programs. But this is difficult to do and success has been limited.

More problems with translation: 
- Some coding constructs can be recognized by an automatic program generator, and converted to a parallel construct.
- However, it is likely that the results will be a very inefficient program.
- Sometimes the best parallel solution is to step back and devise an entirely new algorithm.

Example: compute n values and add them together
Serial Solution:

    sum = 0;
    for (i = 0; i < n; i ++){
		x = Compute_next_value();
		sum += x;
	}

Suppose we have p cores, p << n. Each core performs a partial sum of approximately n/p values.

    my_sum = 0;
    my_first_i = ;
    my_last_i = ;
    for (my_i = my_first_i; my_i < my_last_i; my_i++){
	    my_x = Compute_next_value();
	    my_sum += my_x;}

After each core completes execution of the code, a private variable my_sum contains the sum of the values computed by its calls to Compute_next_value(). Once all the cores are done computing their private my_sum, they form a global sum by sending results to a designated “master” core which adds the final result.

    If (I'm the master core){
	    sum = my_x;
	    for each core other than myself{
		    receive value from core;
		    sum += value;
		}
	} else {
		send my_x to the master;
	}

To have a better parallel algorithm, do not make the master core do all the work. Share it among the other cores. Pair the cores so that core 0 adds its result with core1's result. Core 2 adds its result with core 3's result, etc. Work with odd and even numbered pairs of cores.

Now repeat the process with only the evenly ranked cores. Core 0 adds result from core 2. Core 4 adds the result from core 6, etc. Now cores divisible by 4 repeat the process, and so forth, until core 0 has the final result.

In the first example, the master core performs 7 receives and 7 additions. In the second example, the master core performs 3 receives and 3 additions. [Know why? Refer to P26] The improvement is more than a factor of 2. 

The difference is more dramatic with a larger number of cores. If we have 1000 cores, we may improve by factor of 100.

**How do we write parallel programs?**
- `Task parallelism` Partition various tasks solving the problem among the cores. Tasks: 1 receiving and 2 addition.

    If (I'm the master core) {
        sum = my_x;
        for each core other than myself{
    	    receive value from core;
    	    sum += value;
    	 }
    } else send my_x to the master; 

- `Data parallelism` Partition the data used in solving the problem among the cores. Each core carries out similar operations on its part of the data.

    sum = 0;
    for (i = 0; i < n; i++){
        x = Compute_next_value();
        sum += x;
    }

### Coordination
- Cores usually need to coordinate their work.
- `Communication` one or more cores send their current partial sums to another core
- `Load balancing` share the work evenly among the cores so that one is not heavily loaded.
- `Synchronization` because each core works at its own pace, make sure cores do not get too far ahead of the rest

### Types of parallel systems
- Shared-memory
	- The cores can share access to the computer's memory
	- Coordinate the cores by having them examine and update shared memory locations
- Distributed-memory
	- Each core has its own, private memory
	- The cores must communicate explicitly by sending messages across a network.

### Terminology
- `Concurrent computing` in a program multiple tasks can be in progress at any instant.
- `Parallel computing` in a program multiple tasks cooperate closely to solve a problem.
- `Distributed computing` a program may need to cooperate with other programs to solve a problem.

## Review on Computer Hardware and Operating Systems

### The Von Neumann Architecture

**Main memory**
 - It is a collection of **locations**, each of which is capable of storing both instructions and data. 
- Every location consists of an **address**, which is used to access the location, and the **contents** of the location.

**Central Processing Unit (CPU)**
- Control unit - responsible for deciding which instruction in a program should be executed
- Arithmetic and Logic Unit (ALU) - responsible for executing the actual instructions.

Key Terms:
- **Register** very fast storage, part of the CPU
- **Program counter** stores address of the next instruction to be executed
- **Bus** wires that connect the CPU and memory.

`The Von Neumann Bottleneck`

The separation of memory and CPU
- The limited storage capacity of the CPU means that large amounts of data and instructions must be transferred from the memory
- The interconnect determines the rate at which instructions and data can be accessed
- The CPU can execute instructions orders of magnitude faster than memory access

`Process`
- An instance of a computer program that is being executed.
- Components:
	- The executable machine language program
	- A block of memory
	- Descriptors of resources the OS has allocated to the process
	- Security information
	- Information about the state of the process

Multitasking
- Gives the illusion that a single processor system is running multiple programs simultaneously.
- Each process takes turns running. `time slice`
- After its time is up, it waits until it has a turn again. `blocks`

Threading
- Threads are contained within processes
- They allow programmers to divide their programs into (more or less) independent tasks
- The hope is that *when one thread blocks because it is waiting on a resource, another will have work to do and can run*

A process and two threads: the "master" thread
- starting a thread is called `forking`
- terminating a thread is called `joining`

### Modifications to the Von Neumann Model

`Caches`
- A collection of memory locations that can be accessed in less time than some other memory locations
- A CPU cache is typically located on the same chip, or one that can be accessed much faster than ordinary memory

`Locality` The same or nearby locations are accessed frequently
`Spatial locality`: accessing a nearby location
`Temporal locality`: accessing in the near future

Levels of Caches, Cache Hit, Cache Miss

When a CPU writes data to cache, the value in cache may be inconsistent with the value in the main memory. 
- `Write-through` caches handle this by updating the data in main memory at the time it is written to cache.
- `Write-back` caches mark data in the cache as dirty. When the cache line is replaced by a new cache line from memory, the dirty line is written to memory.

**Cache Mapping**
- `Full associative` a new line can be placed at any location in the cache.
- `Direct mapped` each cache line has a unique location in the cache to which it will be assigned
- `n-way set associative` each cache line can be place in one of n different locations in the cache.

**Cache Eviction**
Caches are much smaller than main memory. When the cache is full, bringing a new line in memory needs to replace or evict a line in the cache. Common cache eviction policies include **LRU/MRU (Least/Most Recently Used)** and **LFU (Least Frequently Used)**.

### Virtual Memory
- If we run a very large program or a program that accesses very large data sets, all of the instructions and data may not fit into main memory.
- Virtual memory functions as a cache for secondary storage.
- `Swap space` an area of secondary storage that keeps the inactive (parts of) running programs.
- `Pages` blocks of data and instructions. Most systems have a fixed page size that currently ranges from 4 to 16 kilobytes.

When a program is compiled its pages are assigned `Virtual Page Numbers`

When the program is run, a table is created that maps the virtual page numbers to physical addresses.

A `page table` is used to translate the virtual address into a physical address.

Virtual Address Divided into Virtual Page Number and Byte Offset

`Translation-lookaside Buffer` TLB
- Using a page table has the potential to significantly increase each program's overall run-time.
- TLB is a special address translation cache in the processor.
- It caches a small number of entries (typically 16–512) from the page table in very fast memory.
- `Page Fault` attempting to access a valid physical address for a page in the page table but the page is only stored on disk.

`Instruction Level Parallelism (ILP)`
- Attempts to improve processor performance by having multiple processor components or functional units simultaneously executing instructions.
- `Pipelining`  functional units are arranged in stages.
	- Divide the floating point adder into 7 separate pieces of hardware or functional units.
	- First unit fetches two operands, second unit compares exponents, etc.
	- Output of one functional unit is input to the next.
	- One floating point addition still takes 7 nanoseconds.
	- But 1000 floating point additions now takes 1006 nanoseconds!
- `Multiple issue` multiple instructions can be simultaneously initiated.
	- Multiple issue processors replicate functional units and try to simultaneously execute different instructions in a program.
	- `static multiple issue` functional units are scheduled at compile time.
	- `dynamic multiple issue` functional units are scheduled at run-time. `superscalar` 

`Speculation`
- In order to make use of multiple issue, the system must find instructions that can be executed simultaneously.
- In speculation, the compiler or the processor makes a guess about an instruction, and then executes the instruction on the basis of the guess.

`Hardware multithreading`
- There aren't always good opportunities for simultaneous execution of different threads.
- Hardware multithreading provides a means for systems to continue doing useful work when the task being currently executed has stalled. Ex., the current task has to wait for data to be loaded from memory.
- `Fine-grained` the processor switches between threads after each instruction, skipping threads that are stalled
	- Pros: potential to avoid wasted machine time due to stalls
	- Cons: a thread that is ready to execute a long sequence of instructions may have to wait to execute every instruction.
- `Coarse-grained` only switches threads that are stalled waiting for a time-consuming operation to complete.
	- Pros: switching threads doesn't need to be nearly instantaneous
	- Cons: the processor can be idled on shorter stalls, and thread switching will also cause delays.
- `







<!--stackedit_data:
eyJoaXN0b3J5IjpbNzA0OTAwNTE2LDEzNDEyNTE2NTUsMTE3Nj
g1NDQzOSwtMTQwMDc1MDczOCwtMjU5MTA1ODM5LC0xMjM5Nzg2
NzM5LC04MjUxMzM2ODIsLTE5MzQwNzAxMTksMTgyNzI5ODA2MC
wtNTA3MTgzNTg1LC0xOTE1NjkyMDQ1LC0yMDY5NTE2MTM2LDE1
ODI1NzE3NDksMTM2NDM2NzkzNF19
-->