


> Written with [StackEdit](https://stackedit.io/).
# COMP4211 Intro to Machine Learning

## Introduction [Jan 31]

Three major problem types:
- Supervised Learning: well-posed
- Unsupervised leanring: ill-posed
- Reinforcement Learning: well-posed but less mature

Conventional Programming vs. Supervised Learning Difference

### Supervised Learning
Training set $\mathcal{S} = \{(x^{(l)}, y^{(l)})\}$ of $N$ labeled examples each of which is in the form of an input-output pair. The problem is to find function $f(\cdot)$ using $\mathcal{S}$. Want $f(x) = y$ to hold also for unseen examples sampled from the same data distribution.

Testing set Unlabeled examples with only input variables are fed one at a time in to function $f$ 

Types of Supervised Learning Problems:
Two main types: `Classification` output is categorical and `Regression` output is continuous-valued
Variations and generalizations:
- Ranking
- Structured prediction
- Active Learning
- Semi-supervised learning

 - [ ] Should you encode categorical data just as 1,2,3? No. Why? 
 A common way to do it is (1,0,0), (0,1,0), (0,0,1)

### Unsupervised Learning
Given a set $\mathcal{S} = \{x^{(l)}\}$ of $N$ unlabeled examples

Unsupervised Learning problems include:
- Dimensionality Reduction
- Clustering
- Density estimation
- Anomaly detection: detecting outliers in the examples

### Reinforcement Learning

A reinforcement learning agent learns via interacting with the environment
Goal: Maximize rewards by learning to take appropriate actions at different states

## Feb 12

[Notes about Linear Regression on paper]

### Nonlinear Extension

- Applying a nonlinear regression function to the original input
- Explicitly adding more input dimensions (which depend nonlinearly on the original input dimensions) and applying linear regression to the expanded input. [**Force linearity by adding dimension**]
- Implicitly transforming the original input nonlinearly to a new space and applying a linear model to the transformed input

**Polynomial Regression**
- One common approach is to introduce higher-order terms as additional input dimensions
- Besides adding higher-order terms, more general transformations of the original input dimensions may also be introduced as additional input dimensions.
- Very often, feature engineering that uses domain knowledge to deﬁne application-speciﬁc features is applied.
- The method of least squares for linear regression can also be used here (for both polynomial regression and more general extensions) to obtain a closed-form solution.

**Model Overfitting**
- If the training set is small compared with the number of params, overfitting may occur. [How to quantify this?] *One way to prevent overfitting is to use less params.*
- When overfitting occurs, the parameters often have large magnitude
- One common solution is to *prevent the params from growing excessively large in magnitude*

**Regularization**
Regularization is an approach which modiﬁes the original loss function by adding one or more penalty terms, called regularizers, that penalize large parameter magnitudes.

$L_2$ regularization is also called Tikhonov regularization. Ridge Regression.
$\lambda > 0$ is called the regularization parameter.

In practice, not regularizing $w_0$ usually gives better result because $w_0$ serves as an oﬀset which is not multiplied with any input variable.

- [ ] Know the form of Ridge Regression and Close-form solution

Though not the only method, `cross validation` is commonly used by training a model (using a regularized loss function with a specific value of λ) on a training set and validating the trained model on a separate validation set (which mimics the test set not available during model training).

Overfitting: May occur if $\lambda$ is too small, small training error, large validation error
Underfitting: May occur if $\lambda$ is too large, large training error, large validation error

$L_p$ Norm: $||v||_p = (\sum |v_i|^p)^{1/p}$

L_1 regression is called LASSO, 

 


<!--stackedit_data:
eyJoaXN0b3J5IjpbLTUwMjU3MTQzOSwxNjQyMDkyMzUzLC0xNz
gyNjIwMDEsLTE3NTg2NTY1NzEsMTk3NTkzOTkyNywtOTc1ODM2
NzQ5LDQ3NjIwOTI2OF19
-->